import json
import logging
import asyncio
import traceback
from typing import List, Dict, Any, Optional, Callable

from core.browser_controller import BrowserController
from core.web_searcher import WebSearcher
from core.data_extractor import DataExtractor
from core.ollama_handler import OllamaHandler
import settings

logger = logging.getLogger(__name__)

def get_browser_instance():
    """ì‹±ê¸€í„´ ë¸Œë¼ìš°ì € ì»¨íŠ¸ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not hasattr(get_browser_instance, "_instance") or get_browser_instance._instance is None:
        get_browser_instance._instance = BrowserController()
    return get_browser_instance._instance

async def extract_web_content(url: str, task_description: str) -> dict:
    """
    ì›¹í˜ì´ì§€ì˜ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        url: ì›¹í˜ì´ì§€ URL
        task_description: ì¶”ì¶œ ì‘ì—…ì— ëŒ€í•œ ì„¤ëª…
        
    Returns:
        ì¶”ì¶œëœ ë‚´ìš©ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
    """
    logger.info(f"Extracting content from {url}")
    try:
        browser = get_browser_instance()
        result = await browser.browse_website(
            url=url,
            action="extract_text",  # ëª…ì‹œì ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì•¡ì…˜ ì§€ì •
            close_browser=False  # ë¸Œë¼ìš°ì €ë¥¼ ê³„ì† ì¬ì‚¬ìš©
        )
        
        # ë¸Œë¼ìš°ì§• ê²°ê³¼ê°€ ì„±ê³µì ì¸ì§€ í™•ì¸
        if result.get("status") != "success":
            error_msg = result.get("error_message", "Unknown error")
            logger.error(f"Failed to extract content: {error_msg}")
            return {
                "success": False, 
                "error": f"ë¸Œë¼ìš°ì§• ì‹¤íŒ¨: {error_msg}", 
                "content": None,
                "url": url,
                "title": result.get("page_title", "")
            }
        
        # í…ìŠ¤íŠ¸ ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸
        content = result.get("data", {}).get("text_content", "")
        if not content.strip():
            logger.warning(f"No text content extracted from {url}")
            # ì¼ë°˜ ì½˜í…ì¸  ì¶”ì¶œ ì‹œë„
            result = await browser.browse_website(
                url=url,
                action=None,  # ê¸°ë³¸ get_content ì•¡ì…˜ ì‚¬ìš©
                close_browser=False
            )
            content = result.get("data", {}).get("text_content", "")
            
        # ì—¬ì „íˆ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬
        if not content.strip():
            logger.error(f"Failed to extract any content from {url}")
            return {
                "success": False, 
                "error": "ì¶”ì¶œëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤", 
                "content": None,
                "url": url,
                "title": result.get("page_title", "")
            }
            
        # ì„±ê³µì ìœ¼ë¡œ ë‚´ìš©ì„ ì¶”ì¶œí•œ ê²½ìš°
        return {
            "success": True,
            "content": content,
            "url": result.get("final_url", url),
            "title": result.get("page_title", ""),
            "extracted_with": result.get("data", {}).get("used_selector", "default")
        }
            
    except Exception as e:
        logger.exception(f"Error extracting content from {url}: {e}")
        return {
            "success": False,
            "error": f"ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}",
            "content": None,
            "url": url,
            "title": ""
        }

class AgentPipeline:
    def __init__(self, streamlit_status_callback=None):
        self.web_searcher = WebSearcher()
        self.browser_controller = get_browser_instance()
        self.data_extractor = DataExtractor()
        self.llm_handler = OllamaHandler()
        # ìƒíƒœ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ì½œë°± í•¨ìˆ˜
        self._status_callback = streamlit_status_callback

    def _update_status(self, message: str):
        """ìƒíƒœ ë©”ì‹œì§€ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. Streamlit UIë‚˜ ë¡œê·¸ì— í‘œì‹œë©ë‹ˆë‹¤."""
        logger.info(message)
        if self._status_callback:
            self._status_callback(message)

    async def _execute_tool_call(self, tool_name: str, tool_args: dict, collected_data_for_all_blogs: list):
        """LLMì´ ìš”ì²­í•œ ë„êµ¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        self._update_status(f"ğŸ› ï¸ ë„êµ¬ ì‹¤í–‰ ì¤‘: {tool_name} (ì¸ì: {tool_args})")

        if tool_name == "search_web_for_blogs":
            keyword = tool_args.get("keyword")
            if not keyword: 
                return json.dumps({
                    "status": "error",
                    "message": "search_web_for_blogs ë„êµ¬ì— 'keyword' ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤."
                })
                
            search_results = self.web_searcher.search_links(keyword)  # ë™ê¸° í•¨ìˆ˜
            urls = [res["url"] for res in search_results if res.get("url")]
            
            return json.dumps({
                "status": "success",
                "found_urls": urls,
                "summary": f"{len(urls)}ê°œì˜ ì ì¬ì  ë¸”ë¡œê·¸ URLì„ ì°¾ì•˜ìŠµë‹ˆë‹¤."
            })

        elif tool_name == "get_webpage_content_and_interact":
            url = tool_args.get("url")
            fields_to_extract = tool_args.get("fields_to_extract", settings.DATA_FIELDS_TO_EXTRACT)  # ê¸°ë³¸ í•„ë“œ ì‚¬ìš©
            action_details = tool_args.get("action_details")  # ì„ íƒ ì‚¬í•­

            if not url:
                return json.dumps({
                    "status": "error",
                    "message": "get_webpage_content_and_interact ë„êµ¬ì— 'url' ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤."
                })

            # LLMì´ ìš”ì²­í•œ í•„ë“œ + ê¸°ë³¸ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ë„ë¡ êµ¬ì„±
            self._update_status(f"ğŸŒ ì›¹ì‚¬ì´íŠ¸ ë°©ë¬¸ ë° ì›ì‹œ ë°ì´í„° ìˆ˜ì§‘ ì‹œë„: {url}")

            # BrowserControllerëŠ” ì´ì œ í‘œì¤€í™”ëœ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•¨
            action_type = None
            selector = None
            input_text = None
            
            if action_details:
                action_type = action_details.get("action_type")
                selector = action_details.get("selector")
                input_text = action_details.get("input_text")

            raw_result = await self.browser_controller.browse_website(
                url=url,
                action=action_type,
                selector=selector,
                input_text=input_text,
                # close_browser=False # ë£¨í”„ ë‚´ì—ì„œëŠ” ë¸Œë¼ìš°ì € ìœ ì§€
            )
            
            if raw_result["status"] == "success":
                self._update_status(f"ğŸ“„ '{url}' ì—ì„œ ì›¹í˜ì´ì§€ ë‚´ìš© ìˆ˜ì‹  ì™„ë£Œ.")
                
                # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë˜ëŠ” ì•¡ì…˜ ê²°ê³¼ í¬í•¨
                result = {
                    "status": "success",
                    "url": url,
                    "final_url": raw_result["final_url"],
                    "page_title": raw_result["page_title"],
                    "action_performed": raw_result["action_performed"],
                    "requested_fields": fields_to_extract,
                }
                
                # í…ìŠ¤íŠ¸ ë‚´ìš©ì´ ìˆìœ¼ë©´ í¬í•¨
                if "text_content" in raw_result["data"]:
                    result["text_content"] = raw_result["data"]["text_content"]
                # ë©”ì‹œì§€ê°€ ìˆìœ¼ë©´ í¬í•¨
                elif "message" in raw_result["data"]:
                    result["message"] = raw_result["data"]["message"]
                
                return json.dumps(result)
            else:
                self._update_status(f"âš ï¸ '{url}' ì ‘ê·¼ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {raw_result['error_message']}")
                return json.dumps({
                    "status": "error",
                    "url": url,
                    "message": f"ì›¹ì‚¬ì´íŠ¸ ì ‘ê·¼ ì‹¤íŒ¨: {raw_result['error_message']}"
                })

        elif tool_name == "extract_blog_fields_from_text":
            text_content = tool_args.get("text_content")
            original_url = tool_args.get("original_url")
            
            if not text_content or not original_url:
                return json.dumps({
                    "status": "error",
                    "message": "extract_blog_fields_from_text ë„êµ¬ì— 'text_content'ì™€ 'original_url' ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤."
                })

            self._update_status(f"âœï¸ '{original_url}'ì˜ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ ì‹œë„ (LLM í˜¸ì¶œ)...")
            
            # LLMì—ê²Œ íŠ¹ì • í•„ë“œ ì¶”ì¶œì„ ìœ„í•œ ëª…í™•í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            extraction_system_prompt = (
                f"You are an expert data extractor. From the following text content, "
                f"which was obtained from the URL '{original_url}', "
                f"extract these specific fields: {', '.join(settings.DATA_FIELDS_TO_EXTRACT)}. "
                f"Return your findings as a single, well-formed JSON object where keys are the field names. "
                f"If a field's value cannot be found in the text, use the string 'Not Found' as its value. "
                f"For dates, try to use YYYY-MM-DD format if possible, otherwise keep the original format. "
                f"For 'average_visitors', if specific numbers are not present, record any textual hints found (e.g., 'thousands of readers monthly')."
            )
            extraction_user_prompt = text_content

            extraction_messages = [
                {"role": "system", "content": extraction_system_prompt},
                {"role": "user", "content": extraction_user_prompt}
            ]
            
            # LLM í˜¸ì¶œ (JSON í˜•ì‹ ì‘ë‹µ ìš”ì²­)
            llm_response = self.llm_handler.chat_with_ollama_for_tools(
                 extraction_messages,
                 [] # ë„êµ¬ ì—†ì´ í…ìŠ¤íŠ¸ ìƒì„±ë§Œ ìš”ì²­
            )
            extracted_json_string = llm_response.get("content", "{}")
            logger.debug(f"LLM extraction response for {original_url}: {extracted_json_string}")

            try:
                # LLMì´ ë°˜í™˜í•œ JSON ë¬¸ìì—´ íŒŒì‹± ì‹œë„
                # JSON ë¬¸ìì—´ì´ ì‹¤ì œ JSON ê°ì²´ë¥¼ í¬í•¨í•˜ì§€ë§Œ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ìˆì„ ìˆ˜ ìˆìŒ
                # ê°€ì¥ ë°”ê¹¥ìª½ ì¤‘ê´„í˜¸({})ë§Œ ì¶”ì¶œí•˜ì—¬ íŒŒì‹± ì‹œë„
                import re
                
                # JSON ê°ì²´ ì¶”ì¶œ ì‹œë„
                json_pattern = r'(\{[\s\S]*\})'
                json_match = re.search(json_pattern, extracted_json_string)
                
                if json_match:
                    json_str_cleaned = json_match.group(1)
                    try:
                        extracted_info_dict = json.loads(json_str_cleaned)
                    except json.JSONDecodeError:
                        # ì¤‘ê´„í˜¸ ì•ˆì˜ ë‚´ìš©ì„ ì¶”ì¶œí–ˆì§€ë§Œ ì—¬ì „íˆ íŒŒì‹±í•  ìˆ˜ ì—†ëŠ” ê²½ìš°
                        logger.warning(f"ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œí•œ JSONë„ íŒŒì‹± ì‹¤íŒ¨: {json_str_cleaned}")
                        # ì›ë³¸ ë¬¸ìì—´ë¡œ ë‹¤ì‹œ ì‹œë„
                        extracted_info_dict = json.loads(extracted_json_string)
                else:
                    # ì •ê·œì‹ìœ¼ë¡œ JSON ê°ì²´ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ì›ë³¸ ë¬¸ìì—´ë¡œ ì‹œë„
                    extracted_info_dict = json.loads(extracted_json_string)
                
                # DataExtractorë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ë°ì´í„° êµ¬ì¡°í™” ë° ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                structured_blog_info = self.data_extractor.structure_blog_info(extracted_info_dict, original_url)
                collected_data_for_all_blogs.append(structured_blog_info) # ì´ ë¶€ë¶„ì´ ì¤‘ìš”: êµ¬ì¡°í™”ëœ ë°ì´í„° ì €ì¥
                
                self._update_status(f"âœ… ì •ë³´ ì¶”ì¶œ ë° ì €ì¥ ì™„ë£Œ: {original_url} -> {structured_blog_info.get('blog_name', 'Unknown')}")
                
                return json.dumps({
                    "status": "success",
                    "message": f"'{original_url}'ì—ì„œ '{structured_blog_info.get('blog_name', 'Unknown')}' ì •ë³´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.",
                    "extracted_fields": structured_blog_info
                })
            except Exception as e:
                logger.error(f"JSON íŒŒì‹± ë˜ëŠ” ë°ì´í„° êµ¬ì¡°í™” ì˜¤ë¥˜: {str(e)}", exc_info=True)
                self._update_status(f"âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                return json.dumps({
                    "status": "error",
                    "message": f"ì¶”ì¶œëœ ì •ë³´ë¥¼ JSONìœ¼ë¡œ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}",
                    "raw_text": extracted_json_string[:200] + "..." if len(extracted_json_string) > 200 else extracted_json_string
                })
        else:
            logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë„êµ¬: {tool_name}")
            return json.dumps({
                "status": "error",
                "message": f"ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {tool_name}"
            })

    async def run_agent_for_keywords(self, initial_keywords: list):
        self._update_status("ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸ ì‹œì‘...")
        
        # ìˆ˜ì§‘ëœ ëª¨ë“  ë¸”ë¡œê·¸ ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        collected_data_for_all_blogs = []
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = f"""You are an expert blog researcher assistant who specializes in finding and extracting information from blogs.
Your task is to search for blogs related to given keywords, visit their websites, and extract specific information.

Follow this systematic process:
1. Search for blogs related to the given keywords using the search_web_for_blogs tool.
2. For each promising blog URL found:
   a. Visit the website using get_webpage_content_and_interact
   b. Extract the text content from the page
   c. Use the extract_blog_fields_from_text tool to analyze the text and extract these specific fields:
      {', '.join(settings.DATA_FIELDS_TO_EXTRACT)}

Important guidelines:
- Focus on blogs that clearly belong to individual bloggers or small businesses, not large corporate/news sites.
- Visit at least 3-5 different blogs to gather diverse information.
- If a page doesn't clearly contain blog information, move on to another URL.
- Ensure all extracted data is stored properly by confirming the success status of tool responses.
- If some fields cannot be found for a blog, it's okay to proceed with partial information.

You have access to these tools:
- search_web_for_blogs: Find relevant blog URLs based on keywords
- get_webpage_content_and_interact: Visit websites and extract their content
- extract_blog_fields_from_text: Analyze text to extract specific blog information fields

After completing the research, provide a summary of how many blog details you successfully collected and any challenges encountered.
"""

        messages_history = [{"role": "system", "content": system_prompt}]

        # ì´ˆê¸° ì‚¬ìš©ì ë©”ì‹œì§€ (í‚¤ì›Œë“œ ì „ë‹¬)
        user_query = f"ë‹¤ìŒ í‚¤ì›Œë“œì— ëŒ€í•œ ë¸”ë¡œê·¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”: {', '.join(initial_keywords)}. ê° ë¸”ë¡œê·¸ì—ì„œ {', '.join(settings.DATA_FIELDS_TO_EXTRACT)} ì •ë³´ë¥¼ ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤."
        messages_history.append({"role": "user", "content": user_query})

        max_turns = settings.AGENT_MAX_TURNS  # ì˜ˆ: 10-15íšŒ, ì„¤ì • íŒŒì¼ì— ì¶”ê°€ í•„ìš”

        try:
            try:
                await self.browser_controller._ensure_browser()  # íŒŒì´í”„ë¼ì¸ ì‹œì‘ ì‹œ ë¸Œë¼ìš°ì € í•œë²ˆ ì¼¬
                self._update_status("ğŸŒ ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì™„ë£Œ.")
            except RuntimeError as e_browser:
                self._update_status(f"âš ï¸ ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e_browser}")
                # ëª©í‘œ: ì‹œìŠ¤í…œì´ ë¸Œë¼ìš°ì € ë¬¸ì œì—ë„ ë¶ˆêµ¬í•˜ê³  ê³„ì† ì‘ë™í•˜ë„ë¡
                # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ ê³„ì† ì§„í–‰ (ë‚˜ì¤‘ì— ê° ì‘ì—…ì—ì„œ ë¸Œë¼ìš°ì € ì¬ì´ˆê¸°í™” ì‹œë„)
            
            # ì—ì´ì „íŠ¸ ë£¨í”„ ì‹œì‘
            turn = 0
            agent_complete = False
            
            while not agent_complete and turn < max_turns:
                turn += 1
                self._update_status(f"ğŸ”„ ì—ì´ì „íŠ¸ í„´ #{turn}/{max_turns} ì‹¤í–‰ ì¤‘...")
                
                # LLM í˜¸ì¶œ
                tools_for_llm = settings.get_tools_for_ollama()
                
                try:
                    llm_response = self.llm_handler.chat_with_ollama_for_tools(messages_history, tools_for_llm)
                except Exception as e_llm:
                    logger.error(f"LLM í˜¸ì¶œ ì˜¤ë¥˜: {e_llm}", exc_info=True)
                    self._update_status(f"âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e_llm}")
                    # ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•˜ê³  ë‹¤ìŒ í„´ìœ¼ë¡œ ì§„í–‰
                    messages_history.append({
                        "role": "assistant",
                        "content": f"ì£„ì†¡í•©ë‹ˆë‹¤, ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e_llm}. ë‹¤ì‹œ ì‹œë„í•˜ê² ìŠµë‹ˆë‹¤."
                    })
                    continue
                
                # ë„êµ¬ ì‚¬ìš© ìš”ì²­ì„ í™•ì¸
                if "tool_calls" in llm_response and llm_response["tool_calls"]:
                    tool_calls = llm_response["tool_calls"]
                    self._update_status(f"ğŸ§° LLMì´ ë„êµ¬ ì‚¬ìš© ìš”ì²­: {len(tool_calls)}ê°œ")
                    
                    # LLMì˜ ìƒê°/ê³„íšì„ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                    if "content" in llm_response and llm_response["content"]:
                        messages_history.append({
                            "role": "assistant", 
                            "content": llm_response["content"],
                            "tool_calls": [
                                {
                                    "id": tc["id"],
                                    "type": "function",
                                    "function": {"name": tc["function"]["name"], "arguments": tc["function"]["arguments"]}
                                }
                                for tc in tool_calls
                            ]
                        })
                    
                    # ëª¨ë“  ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬
                    for tool_call in tool_calls:
                        try:
                            # ë„êµ¬ ì´ë¦„ê³¼ ì¸ì ì¶”ì¶œ
                            tool_name = tool_call["function"]["name"]
                            tool_args_str = tool_call["function"]["arguments"]
                            
                            try:
                                # JSON ë¬¸ìì—´ì„ íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                                tool_args = json.loads(tool_args_str) if tool_args_str else {}
                            except json.JSONDecodeError:
                                logger.warning(f"ë„êµ¬ ì¸ì íŒŒì‹± ì‹¤íŒ¨: {tool_args_str}")
                                tool_args = {"error": "Invalid JSON arguments", "raw_args": tool_args_str}
                            
                            # ë„êµ¬ ì‹¤í–‰
                            tool_result = await self._execute_tool_call(
                                tool_name, tool_args, collected_data_for_all_blogs
                            )
                            
                            # ë„êµ¬ ê²°ê³¼ë¥¼ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                            messages_history.append({
                                "role": "tool",
                                "tool_call_id": tool_call["id"],
                                "name": tool_name,
                                "content": tool_result
                            })
                            
                        except Exception as e_tool:
                            logger.error(f"ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e_tool}", exc_info=True)
                            self._update_status(f"âš ï¸ ë„êµ¬ í˜¸ì¶œ ì˜¤ë¥˜: {e_tool}")
                            
                            # ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                            messages_history.append({
                                "role": "tool",
                                "tool_call_id": tool_call["id"],
                                "name": tool_call["function"]["name"],
                                "content": json.dumps({
                                    "status": "error",
                                    "message": f"ë„êµ¬ í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e_tool)}"
                                })
                            })
                
                else:
                    # ìµœì¢… ìš”ì•½ ì‘ë‹µ (ì‘ì—… ì™„ë£Œ í‘œì‹œ)
                    if "content" in llm_response and llm_response["content"]:
                        messages_history.append({
                            "role": "assistant",
                            "content": llm_response["content"]
                        })
                        
                        # íŠ¹ì • í‚¤ì›Œë“œë¡œ ì™„ë£Œ ì—¬ë¶€ íŒë‹¨ (ì˜ˆë¥¼ ë“¤ì–´, "ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ", "ì‘ì—… ë" ë“±)
                        completion_signals = ["ìˆ˜ì§‘ ì™„ë£Œ", "ì‘ì—… ì™„ë£Œ", "ì •ë³´ ìˆ˜ì§‘ì„ ë§ˆì³¤ìŠµë‹ˆë‹¤", 
                                             "ëª¨ë“  ë¸”ë¡œê·¸", "successfully collected", "completed the research"]
                        if any(signal in llm_response["content"].lower() for signal in completion_signals):
                            agent_complete = True
                            self._update_status("ğŸ ì—ì´ì „íŠ¸ê°€ ì‘ì—… ì™„ë£Œ ì‹ í˜¸ë¥¼ ë³´ëƒˆìŠµë‹ˆë‹¤.")
                
                # ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì¶©ë¶„íˆ ë§ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ (ì˜ˆ: 5ê°œ ì´ìƒì˜ ë¸”ë¡œê·¸ ì •ë³´)
                if len(collected_data_for_all_blogs) >= settings.MINIMUM_BLOGS_TO_COLLECT:
                    self._update_status(f"âœ… ì¶©ë¶„í•œ ë¸”ë¡œê·¸ ì •ë³´ ìˆ˜ì§‘ë¨ ({len(collected_data_for_all_blogs)}ê°œ)")
                    if not agent_complete:  # ì•„ì§ ì—ì´ì „íŠ¸ê°€ ì™„ë£Œ ì‹ í˜¸ë¥¼ ë³´ë‚´ì§€ ì•Šì€ ê²½ìš°
                        messages_history.append({
                            "role": "user",
                            "content": "ì¶©ë¶„í•œ ì •ë³´ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì‘ì—…ì„ ë§ˆë¬´ë¦¬í•˜ê³  ìˆ˜ì§‘í•œ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”."
                        })
                        # í•œ ë²ˆ ë” LLM í˜¸ì¶œí•˜ì—¬ ìš”ì•½ ì–»ê¸°
                        try:
                            summary_response = self.llm_handler.chat_with_ollama_for_tools(
                                messages_history, []  # ë„êµ¬ ì—†ì´ í…ìŠ¤íŠ¸ ìƒì„±ë§Œ ìš”ì²­
                            )
                            if "content" in summary_response and summary_response["content"]:
                                messages_history.append({
                                    "role": "assistant",
                                    "content": summary_response["content"]
                                })
                        except Exception as e_summary:
                            logger.warning(f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e_summary}")
                        
                        agent_complete = True
            
            # ì—ì´ì „íŠ¸ ë£¨í”„ ì¢…ë£Œ í›„
            if agent_complete:
                self._update_status(f"ğŸ‰ ì—ì´ì „íŠ¸ ì‘ì—… ì™„ë£Œ! ìˆ˜ì§‘ëœ ë¸”ë¡œê·¸ ì •ë³´: {len(collected_data_for_all_blogs)}ê°œ")
            else:
                self._update_status(f"âš ï¸ ìµœëŒ€ í„´ ìˆ˜({max_turns}) ë„ë‹¬. í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ë¸”ë¡œê·¸ ì •ë³´: {len(collected_data_for_all_blogs)}ê°œ")
            
            # ìµœì¢… ê²°ê³¼ ë°˜í™˜
            final_structured_blog_data = collected_data_for_all_blogs
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ì—ì„œ ë§ˆì§€ë§‰ ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ì¶œ (ìš”ì•½ ìš©ë„)
            last_assistant_message = ""
            for msg in reversed(messages_history):
                if msg.get("role") == "assistant" and "content" in msg and msg["content"]:
                    last_assistant_message = msg["content"]
                    break

            # ì¶”ê°€ëœ ë¶€ë¶„: ë°ì´í„°ê°€ ë¹„ì—ˆì§€ë§Œ LLMì´ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì „ë‹¬í–ˆëŠ”ì§€ í™•ì¸
            if not final_structured_blog_data:
                self._update_status("ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì—ì„œ ìœ ìš©í•œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
                # ê°€ëŠ¥í•œ URL ëª©ë¡ ì¶”ì¶œ
                urls_found = set()
                for msg in messages_history:
                    if msg.get("role") == "tool" and msg.get("name") == "search_web_for_blogs":
                        try:
                            search_result = json.loads(msg.get("content", "{}"))
                            if search_result.get("found_urls"):
                                urls_found.update(search_result.get("found_urls", []))
                        except:
                            pass
                
                if urls_found:
                    self._update_status(f"{len(urls_found)}ê°œì˜ URLì´ ê²€ìƒ‰ë˜ì—ˆì§€ë§Œ ë°ì´í„° ì¶”ì¶œì€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                    # ë‹¨ìˆœ URL ì •ë³´ë¼ë„ ì €ì¥
                    for url in urls_found:
                        simple_data = {
                            "blog_name": "ì¶”ì¶œ ì‹¤íŒ¨",
                            "url": url,
                            "blog_id": "extraction-failed",
                            "recent_post_date": "Not extracted",
                            "total_posts": "Unknown"
                        }
                        final_structured_blog_data.append(simple_data)

        except Exception as e:
            logger.error(f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            self._update_status(f"âŒ ì—ì´ì „íŠ¸ ì˜¤ë¥˜: {e}")
            
            # ì˜¤ë¥˜ ìƒì„¸ ì •ë³´ ë¡œê¹…
            import traceback
            self._update_status("ì˜¤ë¥˜ ìƒì„¸ ì •ë³´ (ë””ë²„ê¹…ìš©):")
            self._update_status(traceback.format_exc()[:1000])  # ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ ìë¦„
            
            return {
                "success": False,
                "error": str(e),
                "data": collected_data_for_all_blogs if 'collected_data_for_all_blogs' in locals() else [],
                "message_history": messages_history if 'messages_history' in locals() else [],
                "summary": f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            }
        finally:
            # ë¸Œë¼ìš°ì € ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œë„
            try:
                await self.browser_controller.close_all_resources()
                self._update_status("ğŸ§¹ ë¸Œë¼ìš°ì € ë¦¬ì†ŒìŠ¤ ì •ë¦¬ë¨")
            except Exception as e_close:
                logger.error(f"ë¸Œë¼ìš°ì € ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e_close}")
        
        # ìµœì¢… ê²°ê³¼ ë°˜í™˜
        return {
            "success": True,
            "data": final_structured_blog_data,
            "message_history": messages_history,
            "summary": last_assistant_message if 'last_assistant_message' in locals() else "ì‘ì—… ì™„ë£Œ"
        } 