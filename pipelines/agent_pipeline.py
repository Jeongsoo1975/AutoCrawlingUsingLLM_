# pipelines/agent_pipeline.py
import asyncio
import json
import logging
import re
import traceback
from typing import List, Dict, Any, Optional, Callable  # Optional, Callable ì¶”ê°€
from config import settings
from core.llm_handler import LLMHandler
from core.web_searcher import WebSearcher
from core.browser_controller import BrowserController
from core.data_extractor import DataExtractor
# DataWriter ì‚¬ìš©ì„ ê°€ì •í•˜ê³  ìˆ˜ì • (ë§Œì•½ ExcelWriterê°€ ë§ë‹¤ë©´ ì´ ë¶€ë¶„ê³¼ í´ë˜ìŠ¤ ë‚´ self.data_writer ìˆ˜ì • í•„ìš”)
from utils.excel_writer import DataWriter
from tools.tool_definitions import TOOLS_SPEC
# utils.improved_system_promptì—ì„œ í”„ë¡¬í”„íŠ¸ ë¡œë” ê°€ì ¸ì˜¤ê¸°
from utils.improved_system_prompt import get_improved_system_prompt, get_extraction_prompt

logger = logging.getLogger(__name__)


def get_browser_instance():
    """ì‹±ê¸€í„´ ë¸Œë¼ìš°ì € ì»¨íŠ¸ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not hasattr(get_browser_instance, "_instance") or get_browser_instance._instance is None:
        get_browser_instance._instance = BrowserController()
    return get_browser_instance._instance


class AgentPipeline:
    def __init__(self, streamlit_status_callback=None):
        self.llm_handler = LLMHandler()
        self.web_searcher = WebSearcher()
        self.browser_controller = get_browser_instance()
        self.data_extractor = DataExtractor()
        self.data_writer = DataWriter()  # ExcelWriter ëŒ€ì‹  DataWriter ì‚¬ìš©
        self.streamlit_status_callback = streamlit_status_callback

    def _update_status(self, message):
        """Streamlit UIì— ìƒíƒœ ë©”ì‹œì§€ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤ (ì½œë°±ì´ ì œê³µëœ ê²½ìš°)."""
        logger.info(f"Agent Status: {message}")
        if self.streamlit_status_callback:
            self.streamlit_status_callback(message)

    async def _execute_tool_call(self, tool_name: str, tool_args: dict, collected_data_for_all_blogs: list, messages_history=None):
        """LLMì´ ìš”ì²­í•œ ë„êµ¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        self._update_status(f"[TOOL] ë„êµ¬ ì‹¤í–‰ ì¤‘: {tool_name} (ì¸ì: {tool_args})")

        if tool_name == "search_web_for_blogs":
            keyword = tool_args.get("keyword")
            if not keyword:
                return json.dumps({
                    "status": "error",
                    "message": "search_web_for_blogs ë„êµ¬ì— 'keyword' ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤."
                })

            search_results = self.web_searcher.search_links(keyword)
            urls = [res["url"] for res in search_results if res.get("url")]

            return json.dumps({
                "status": "success",
                "found_urls": urls,
                "summary": f"{len(urls)}ê°œì˜ ì ì¬ì  ë¸”ë¡œê·¸ URLì„ ì°¾ì•˜ìŠµë‹ˆë‹¤."
            })

        elif tool_name == "get_webpage_content_and_interact":
            url = tool_args.get("url")
            # settings.DATA_FIELDS_TO_EXTRACTë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
            fields_to_extract = tool_args.get("fields_to_extract", settings.DATA_FIELDS_TO_EXTRACT)
            action_details = tool_args.get("action_details")

            if not url:
                return json.dumps({
                    "status": "error",
                    "message": "get_webpage_content_and_interact ë„êµ¬ì— 'url' ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤."
                })

            # URL ìœ íš¨ì„± ê²€ì‚¬ (ê°„ë‹¨í•œ í˜•íƒœë¡œ í†µì¼)
            if not url.startswith(('http://', 'https://')):
                logger.warning(f"Invalid URL format detected: {url}")
                return json.dumps({
                    "status": "error",
                    "url": url,
                    "message": f"Invalid URL format: {url}. URL must start with http:// or https://"
                })

            self._update_status(f"[WEB] ì›¹ì‚¬ì´íŠ¸ ë°©ë¬¸ ë° ì›ì‹œ ë°ì´í„° ìˆ˜ì§‘ ì‹œë„: {url}")

            action_type = None
            selector = None
            input_text = None

            if action_details:  # action_detailsê°€ Noneì´ ì•„ë‹ ê²½ìš°ì—ë§Œ ë‚´ë¶€ ê°’ ì ‘ê·¼
                action_type = action_details.get("action_type")
                selector = action_details.get("selector")
                input_text = action_details.get("input_text")

            raw_result = await self.browser_controller.browse_website(
                url=url,
                action=action_type,
                selector=selector,
                input_text=input_text
                # close_browser=False # ë£¨í”„ ë‚´ì—ì„œëŠ” ë¸Œë¼ìš°ì € ìœ ì§€ (AgentPipelineì—ì„œ ê´€ë¦¬)
            )

            if raw_result["status"] == "success":
                # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì½˜í…ì¸  í’ˆì§ˆ ê²€ì¦
                text_content = raw_result.get("data", {}).get("text_content", "")
                text_length = len(text_content.strip()) if text_content else 0
                
                # ì»¨í…ì¸  í’ˆì§ˆ ê²€ì¦ ë° ê²½ê³ 
                content_quality_warning = ""
                if text_length == 0:
                    content_quality_warning = "âš ï¸ ë¹ˆ ì»¨í…ì¸ ê°€ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤."
                    logger.warning(f"Empty content extracted from {url}")
                elif text_length < 100:
                    content_quality_warning = f"âš ï¸ ë§¤ìš° ì§§ì€ ì»¨í…ì¸ ê°€ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤ ({text_length} ë¬¸ì)."
                    logger.warning(f"Very short content extracted from {url}: {text_length} characters")
                elif text_length < 300:
                    content_quality_warning = f"âš ï¸ ì§§ì€ ì»¨í…ì¸ ê°€ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤ ({text_length} ë¬¸ì)."
                    logger.info(f"Short content extracted from {url}: {text_length} characters")
                else:
                    logger.info(f"Good content extracted from {url}: {text_length} characters")
                    # ğŸš€ ê°•ì œ ë„êµ¬ í˜¸ì¶œ: LLMì´ extract_blog_fields_from_textë¥¼ í˜¸ì¶œí•˜ì§€ ì•ŠëŠ” ë¬¸ì œ í•´ê²°
                    self._update_status("ğŸš€ ì¢‹ì€ ì»¨í…ì¸  ê°ì§€! extract_blog_fields_from_text ë„êµ¬ ê°•ì œ í˜¸ì¶œ...")
                    
                    try:
                        # extract_blog_fields_from_text ë„êµ¬ ì§ì ‘ í˜¸ì¶œ
                        extract_result = await self._execute_tool_call(
                            "extract_blog_fields_from_text",
                            {
                                "text_content": text_content[:5000],  # ì²˜ìŒ 5000ìë§Œ ì‚¬ìš©
                                "original_url": url
                            },
                            collected_data_for_all_blogs,
                            messages_history
                        )
                        
                        if extract_result:
                            extract_result_obj = json.loads(extract_result)
                            if extract_result_obj.get('status') == 'success':
                                self._update_status("âœ… ê°•ì œ ë„êµ¬ í˜¸ì¶œë¡œ ë¸”ë¡œê·¸ ë°ì´í„° ì¶”ì¶œ ì„±ê³µ!")
                                logger.info(f"[FORCE EXTRACT] Successfully extracted blog data: {extract_result_obj.get('extracted_blog_name', 'Unknown')}")
                            else:
                                self._update_status("âš ï¸ ê°•ì œ ë„êµ¬ í˜¸ì¶œ ì‹¤íŒ¨")
                                logger.warning(f"[FORCE EXTRACT] Failed: {extract_result_obj.get('message', 'Unknown error')}")
                        
                    except Exception as e:
                        self._update_status(f"âŒ ê°•ì œ ë„êµ¬ í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                        logger.error(f"[FORCE EXTRACT] Error during forced tool call: {e}", exc_info=True)
                
                self._update_status(f"[PAGE] '{url}' ì—ì„œ ì›¹í˜ì´ì§€ ë‚´ìš© ìˆ˜ì‹  ì™„ë£Œ. í…ìŠ¤íŠ¸ ê¸¸ì´: {text_length} ë¬¸ì")
                if content_quality_warning:
                    self._update_status(content_quality_warning)

                result = {
                    "status": "success",
                    "url": url,  # ìš”ì²­ëœ URL
                    "final_url": raw_result["final_url"],  # ì‹¤ì œ ë„ë‹¬í•œ URL
                    "page_title": raw_result["page_title"],
                    "action_performed": raw_result["action_performed"],
                    "requested_fields": fields_to_extract,  # LLMì´ ìš”ì²­í•œ í•„ë“œ ì •ë³´ í¬í•¨
                    "content_quality": {
                        "text_length": text_length,
                        "quality_status": "good" if text_length >= 300 else "short" if text_length >= 100 else "very_short" if text_length > 0 else "empty",
                        "warning": content_quality_warning,
                        "used_selector": raw_result.get("data", {}).get("used_selector", "unknown")
                    }
                }
                
                # browse_website ê²°ê³¼ì˜ data í•„ë“œì—ì„œ text_content ë˜ëŠ” message ê°€ì ¸ì˜¤ê¸°
                if "text_content" in raw_result.get("data", {}):
                    result["text_content"] = raw_result["data"]["text_content"]
                    
                    # ë¹ˆ ì»¨í…ì¸ ë‚˜ ë§¤ìš° ì§§ì€ ì»¨í…ì¸ ì¸ ê²½ìš° LLMì—ê²Œ ì¶”ê°€ ì •ë³´ ì œê³µ
                    if text_length < 100:
                        result["content_extraction_note"] = f"ì¶”ì¶œëœ ì»¨í…ì¸ ê°€ ë§¤ìš° ì§§ìŠµë‹ˆë‹¤ ({text_length} ë¬¸ì). ì´ URLì—ì„œ ë‹¤ë¥¸ ì…€ë ‰í„°ë¥¼ ì‹œë„í•˜ê±°ë‚˜ ë‹¤ë¥¸ URLì„ ì°¾ì•„ë³´ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”. í˜ì´ì§€ ì œëª©: '{raw_result.get('page_title', 'Unknown')}'"
                        
                        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ì˜ ê²½ìš° ì¶”ê°€ ê°€ì´ë“œë¼ì¸ ì œê³µ
                        if "blog.naver.com" in url:
                            result["naver_blog_note"] = "ë„¤ì´ë²„ ë¸”ë¡œê·¸ì—ì„œ ì»¨í…ì¸  ì¶”ì¶œì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë°”ì¼ ë²„ì „ì´ ì•„ë‹Œ ë°ìŠ¤í¬íƒ‘ ë²„ì „ URLì„ ì‚¬ìš©í•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ë˜ëŠ” ë‹¤ë¥¸ ë¸”ë¡œê·¸ í”Œë«í¼ì„ ì‹œë„í•´ë³´ì„¸ìš”."
                            
                elif "message" in raw_result.get("data", {}):  # ì˜ˆ: í´ë¦­ ì„±ê³µ ë©”ì‹œì§€ ë“±
                    result["message"] = raw_result["data"]["message"]

                return json.dumps(result)
            else:
                self._update_status(f"âš ï¸ '{url}' ì ‘ê·¼ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {raw_result.get('error_message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                return json.dumps({
                    "status": "error",
                    "url": url,
                    "message": f"ì›¹ì‚¬ì´íŠ¸ ì ‘ê·¼ ì‹¤íŒ¨: {raw_result.get('error_message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
                })

        elif tool_name == "extract_blog_fields_from_text":
            text_content = tool_args.get("text_content")
            original_url = tool_args.get("original_url") or tool_args.get("url")  # urlë„ í—ˆìš©
            source_keyword = tool_args.get("source_keyword", "unknown_keyword")  # ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ê°€
            
            # messages_historyê°€ ì œê³µëœ ê²½ìš° í‚¤ì›Œë“œ ë³µêµ¬ ì‹œë„
            if source_keyword == "unknown_keyword" and messages_history:
                # 1. search_web_for_blogs ë„êµ¬ í˜¸ì¶œì—ì„œ í‚¤ì›Œë“œ ì°¾ê¸°
                for msg in reversed(messages_history):
                    if msg.get("role") == "assistant" and msg.get("tool_calls"):
                        for tool_call in msg.get("tool_calls", []):
                            if tool_call.get("function", {}).get("name") == "search_web_for_blogs":
                                try:
                                    args_str = tool_call.get("function", {}).get("arguments", "{}")
                                    # argumentsê°€ ì´ë¯¸ dictì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                                    if isinstance(args_str, dict):
                                        args = args_str
                                    else:
                                        args = json.loads(args_str)
                                    if args.get("keyword"):
                                        source_keyword = args["keyword"]
                                        logger.info(f"[KEYWORD RECOVERY] Found keyword from search tool: {source_keyword}")
                                        break
                                except (json.JSONDecodeError, TypeError):
                                    pass
                        if source_keyword != "unknown_keyword":
                            break
                
                # 2. ì²« ë²ˆì§¸ ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                if source_keyword == "unknown_keyword":
                    for msg in messages_history[:3]:  # ì´ˆê¸° ë©”ì‹œì§€ í™•ì¸
                        if msg.get("role") == "user":
                            content_text = msg.get("content", "")
                            # ë‹¤ì–‘í•œ íŒ¨í„´ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
                            patterns = [
                                r'í‚¤ì›Œë“œ[:\s]*([^\s,ì—ëŒ€í•œê¹Œì§€]+)',
                                r'ë‹¤ìŒ í‚¤ì›Œë“œ[:\s]*([^\s,ì—ëŒ€í•œê¹Œì§€]+)',
                                r'["\']([a-zA-Zê°€-í£]+)["\']',
                                r'([a-zA-Z]+)ì—? ?ëŒ€í•œ',
                                r'([a-zA-Zê°€-í£]+)\s*ì •ë³´'
                            ]
                            for pattern in patterns:
                                keyword_match = re.search(pattern, content_text, re.IGNORECASE)
                                if keyword_match:
                                    candidate = keyword_match.group(1).lower().strip()
                                    if len(candidate) > 1 and candidate not in ['í‚¤ì›Œë“œ', 'ì •ë³´', 'ëŒ€í•œ']:
                                        source_keyword = candidate
                                        logger.info(f"[KEYWORD RECOVERY] Extracted keyword from user message: {source_keyword}")
                                        break
                            if source_keyword != "unknown_keyword":
                                break

            # í…ìŠ¤íŠ¸ ì»¨í…ì¸  í’ˆì§ˆ ë° ìœ íš¨ì„± ê²€ì¦
            if not text_content:  # text_contentëŠ” í•„ìˆ˜
                return json.dumps({
                    "status": "error",
                    "message": "extract_blog_fields_from_text ë„êµ¬ì— 'text_content' ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤."
                })
            if not original_url:  # original_url ë˜ëŠ” urlë„ í•„ìˆ˜
                return json.dumps({
                    "status": "error",
                    "message": "extract_blog_fields_from_text ë„êµ¬ì— 'original_url' ë˜ëŠ” 'url' ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤."
                })
            
            # í…ìŠ¤íŠ¸ ì»¨í…ì¸  ê¸¸ì´ ë° í’ˆì§ˆ ê²€ì¦
            text_content_stripped = text_content.strip()
            text_length = len(text_content_stripped)
            
            if text_length == 0:
                logger.warning(f"Empty text content provided for extraction from {original_url}")
                return json.dumps({
                    "status": "error",
                    "message": f"Empty text content provided for {original_url}. Cannot extract blog information from empty text.",
                    "suggestion": "Try browsing the URL again with different selectors or check if the page loaded correctly."
                })
            
            if text_length < 50:
                logger.warning(f"Very short text content provided for extraction from {original_url}: {text_length} characters")
                return json.dumps({
                    "status": "error", 
                    "message": f"Text content too short for reliable extraction from {original_url} ({text_length} characters).",
                    "text_preview": text_content_stripped[:100],
                    "suggestion": "The extracted text is too short to contain meaningful blog information. Try using different CSS selectors or browse a different URL."
                })

            self._update_status(f"âœï¸ '{original_url}'ì˜ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ ì‹œë„ (LLM í˜¸ì¶œ)... í…ìŠ¤íŠ¸ ê¸¸ì´: {text_length} ë¬¸ì")
            logger.info(f"[EXTRACTION START] URL: {original_url}, Keyword: {source_keyword}, Text Length: {text_length}")
            logger.debug(f"[EXTRACTION DEBUG] Text content length: {len(text_content)} characters")
            logger.debug(f"[EXTRACTION DEBUG] Text preview (first 300 chars): {text_content[:300]}...")

            # ê°œì„ ëœ LLM í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (get_extraction_prompt ì§ì ‘ ì‚¬ìš©)
            extraction_system_prompt = get_extraction_prompt()
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¥¸ ê²½ê³  ë©”ì‹œì§€ ì¶”ê°€
            content_quality_note = ""
            if text_length < 200:
                content_quality_note = f"\n\nâš ï¸ CONTENT WARNING: The provided text is quite short ({text_length} characters). This may indicate:\n1. Poor content extraction due to dynamic loading\n2. Incorrect CSS selectors used for content extraction\n3. Access restrictions, login required, or content behind paywall\n4. The page may not contain the expected blog content\n5. Mobile/responsive version with limited content display\n\nPlease extract what information you can, but note any limitations in your response. If blog information cannot be reliably extracted due to insufficient content, indicate this clearly."
            elif text_length < 500:
                content_quality_note = f"\n\nâš ï¸ Note: The provided text is relatively short ({text_length} characters). Extract available information but be aware of potential content limitations."
            
            extraction_user_prompt = f"Extract information from this text from URL '{original_url}':{content_quality_note}\n\nSource keyword: {source_keyword}\nText length: {text_length} characters\nURL: {original_url}\n\nText content:\n{text_content}"

            extraction_messages = [
                {"role": "system", "content": extraction_system_prompt},
                {"role": "user", "content": extraction_user_prompt}
            ]

            logger.debug(f"[EXTRACTION DEBUG] System prompt length: {len(extraction_system_prompt)} characters")
            logger.debug(f"[EXTRACTION DEBUG] User prompt length: {len(extraction_user_prompt)} characters")

            llm_response = self.llm_handler.chat_with_ollama_for_tools(
                extraction_messages,
                []  # ë„êµ¬ ì—†ì´ í…ìŠ¤íŠ¸ ìƒì„±ë§Œ ìš”ì²­
            )
            extracted_json_string = llm_response.get("content", "{}")
            logger.info(f"[EXTRACTION LLM] Raw LLM response for {original_url}: {extracted_json_string}")
            logger.debug(f"[EXTRACTION LLM] Response length: {len(extracted_json_string)} characters")

            try:
                # ê°œì„ ëœ JSON íŒŒì‹± ë¡œì§ ì‚¬ìš©
                def robust_json_parse(json_string):
                    # 1ë‹¨ê³„: í‘œì¤€ JSON íŒŒì‹±
                    try:
                        return json.loads(json_string)
                    except json.JSONDecodeError:
                        pass
                    # 2ë‹¨ê³„: single quotesë¥¼ double quotesë¡œ ë³€í™˜
                    try:
                        json_compatible = json_string.replace("'", '"')
                        return json.loads(json_compatible)
                    except json.JSONDecodeError:
                        pass
                    # 3ë‹¨ê³„: ast.literal_eval ì‚¬ìš©
                    try:
                        import ast
                        return ast.literal_eval(json_string)
                    except (ValueError, SyntaxError):
                        pass
                    # 4ë‹¨ê³„: ì •ê·œì‹ìœ¼ë¡œ JSON ì¶”ì¶œ í›„ ì¬ì‹œë„
                    json_pattern = r'(\{[\s\S]*\})'
                    json_match = re.search(json_pattern, json_string)
                    if json_match:
                        json_str_cleaned = json_match.group(1).replace("'", '"')
                        try:
                            return json.loads(json_str_cleaned)
                        except json.JSONDecodeError:
                            pass
                    return None

                # ë¨¼ì € ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì²˜ë¦¬
                match_markdown_json = re.search(r'```json\s*(\{[\s\S]*?\})\s*```', extracted_json_string, re.DOTALL)
                if match_markdown_json:
                    json_str_to_parse = match_markdown_json.group(1)
                else:
                    json_str_to_parse = extracted_json_string

                extracted_info_dict = robust_json_parse(json_str_to_parse)
                
                if extracted_info_dict is None:
                    # ìµœí›„ì˜ ìˆ˜ë‹¨: ì›ë³¸ ë¬¸ìì—´ë¡œ ì¬ì‹œë„
                    extracted_info_dict = robust_json_parse(extracted_json_string)
                    if extracted_info_dict is None:
                        raise json.JSONDecodeError("ëª¨ë“  íŒŒì‹± ë°©ë²• ì‹¤íŒ¨", extracted_json_string, 0)
                
                logger.debug(f"ì„±ê³µì ìœ¼ë¡œ íŒŒì‹±ëœ ë°ì´í„°: {extracted_info_dict}")
                logger.debug(f"[EXTRACTION PARSING] Parsed data type: {type(extracted_info_dict)}, keys: {list(extracted_info_dict.keys()) if isinstance(extracted_info_dict, dict) else 'Not a dict'}")

                logger.info(f"[EXTRACTION MAPPING] Starting structure_blog_info for {original_url}")
                structured_blog_info = self.data_extractor.structure_blog_info(extracted_info_dict, original_url)
                logger.info(f"[EXTRACTION MAPPING] Structured result: {structured_blog_info}")
                
                # source_keyword ì •ë³´ë„ ì¶”ê°€
                if 'source_keyword' not in structured_blog_info or not structured_blog_info['source_keyword']:
                    structured_blog_info['source_keyword'] = source_keyword
                    logger.debug(f"[EXTRACTION MAPPING] Added source_keyword: {source_keyword}")
                    
                collected_data_for_all_blogs.append(structured_blog_info)
                logger.info(f"[EXTRACTION SUCCESS] Data added to collection. Total blogs: {len(collected_data_for_all_blogs)}")

                self._update_status(
                    f"âœ… ì •ë³´ ì¶”ì¶œ ë° ì €ì¥ ì™„ë£Œ: {original_url} -> {structured_blog_info.get('blog_name', 'Unknown')}")
                logger.info(f"[EXTRACTION COMPLETE] Final structured data for {original_url}: {structured_blog_info}")
                return json.dumps({
                    "status": "success",
                    "message": f"Successfully extracted and structured data for {original_url}.",
                    "extracted_blog_name": structured_blog_info.get("blog_name", "Unknown"),
                    "extraction_summary": {
                        "blog_name": structured_blog_info.get("blog_name", "Unknown"),
                        "blog_id": structured_blog_info.get("blog_id", "Unknown"),
                        "recent_post_date": structured_blog_info.get("recent_post_date", "Not Found"),
                        "total_posts": structured_blog_info.get("total_posts", "Not Found"),
                        "source_keyword": structured_blog_info.get("source_keyword", "unknown_keyword")
                    }
                })
            except json.JSONDecodeError as e:
                logger.error(f"LLM ì •ë³´ ì¶”ì¶œ ê²°ê³¼ JSON íŒŒì‹± ì‹¤íŒ¨ ({original_url}): {extracted_json_string}. ì˜¤ë¥˜: {e}")
                return json.dumps({
                    "status": "error",
                    "message": f"Failed to parse JSON from LLM's extraction for {original_url}.",
                    "raw_llm_output": extracted_json_string[:500] + ("..." if len(extracted_json_string) > 500 else "")
                })
            except Exception as e_struct:  # DataExtractor.structure_blog_info ë“±ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì˜ˆì™¸
                logger.error(f"DataExtractor ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({original_url}): {e_struct}", exc_info=True)
                return json.dumps({
                    "status": "error",
                    "message": f"Error structuring extracted data for {original_url}: {str(e_struct)}",
                    "raw_llm_output": extracted_json_string[:500] + ("..." if len(extracted_json_string) > 500 else "")
                })

        elif tool_name == "analyze_blog_quality":
            blog_url = tool_args.get("blog_url")
            content_sample = tool_args.get("content_sample", "")
            evaluation_criteria = tool_args.get("evaluation_criteria", ["authority", "freshness", "depth", "relevance"])
            
            if not blog_url:
                return json.dumps({
                    "status": "error",
                    "message": "analyze_blog_quality ë„êµ¬ì— 'blog_url' ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤."
                })
            
            # Gemma3-Toolsì˜ ê³ ê¸‰ ë¶„ì„ ëŠ¥ë ¥ì„ í™œìš©í•œ ë¸”ë¡œê·¸ í’ˆì§ˆ í‰ê°€
            quality_analysis_prompt = f"""
            Analyze the quality of this blog based on the following criteria: {', '.join(evaluation_criteria)}
            
            Blog URL: {blog_url}
            Content Sample: {content_sample[:1000]}...
            
            Provide a detailed quality assessment including:
            1. Authority score (1-10)
            2. Content freshness (1-10)
            3. Content depth (1-10)
            4. Topic relevance (1-10)
            5. Overall quality score (1-10)
            6. Specific strengths and weaknesses
            7. Recommendation (extract/skip)
            
            Return as JSON format.
            """
            
            quality_messages = [{"role": "user", "content": quality_analysis_prompt}]
            quality_response = self.llm_handler.chat_with_ollama_for_tools(quality_messages, [])
            quality_result = quality_response.get("content", "{}")
            
            try:
                quality_data = json.loads(quality_result)
                return json.dumps({
                    "status": "success",
                    "blog_url": blog_url,
                    "quality_analysis": quality_data,
                    "recommendation": quality_data.get("recommendation", "extract")
                })
            except json.JSONDecodeError:
                return json.dumps({
                    "status": "success",
                    "blog_url": blog_url,
                    "quality_analysis": {"raw_analysis": quality_result},
                    "recommendation": "extract"  # ê¸°ë³¸ê°’
                })
                
        elif tool_name == "smart_search_refinement":
            original_keyword = tool_args.get("original_keyword")
            search_results_quality = tool_args.get("search_results_quality")
            target_blog_types = tool_args.get("target_blog_types", [])
            
            if not original_keyword or not search_results_quality:
                return json.dumps({
                    "status": "error",
                    "message": "smart_search_refinement ë„êµ¬ì— 'original_keyword'ì™€ 'search_results_quality' ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤."
                })
            
            # Gemma3-Toolsë¡œ ì§€ëŠ¥ì  ê²€ìƒ‰ ì „ëµ ê°œì„ 
            refinement_prompt = f"""
            Original keyword: {original_keyword}
            Search results quality: {search_results_quality}
            Target blog types: {', '.join(target_blog_types) if target_blog_types else 'Any'}
            
            Based on the search quality assessment, suggest improved search strategies:
            1. Alternative keywords or phrases
            2. More specific search terms
            3. Different search approaches
            4. Platform-specific searches (e.g., "site:medium.com {original_keyword}")
            
            Provide 3-5 concrete suggestions for better search results.
            Return as JSON with 'suggested_keywords' array and 'search_strategy' description.
            """
            
            refinement_messages = [{"role": "user", "content": refinement_prompt}]
            refinement_response = self.llm_handler.chat_with_ollama_for_tools(refinement_messages, [])
            refinement_result = refinement_response.get("content", "{}")
            
            try:
                refinement_data = json.loads(refinement_result)
                return json.dumps({
                    "status": "success",
                    "original_keyword": original_keyword,
                    "search_refinements": refinement_data
                })
            except json.JSONDecodeError:
                return json.dumps({
                    "status": "success", 
                    "original_keyword": original_keyword,
                    "search_refinements": {"raw_suggestions": refinement_result}
                })

        elif tool_name == "finalize_blog_data_collection":
            all_done = tool_args.get("all_tasks_completed", False)
            quality_score = tool_args.get("quality_score", 0)
            recommendations = tool_args.get("recommendations", [])
            
            self._update_status(f"ğŸ ë°ì´í„° ìˆ˜ì§‘ ë§ˆë¬´ë¦¬ ë‹¨ê³„. ìˆ˜ì§‘ëœ ë¸”ë¡œê·¸ ìˆ˜: {len(collected_data_for_all_blogs)}, í’ˆì§ˆ ì ìˆ˜: {quality_score}/10")
            
            # Gemma3-Toolsë¡œ ìµœì¢… ë°ì´í„° í’ˆì§ˆ ê²€ì¦
            if collected_data_for_all_blogs:
                final_analysis_prompt = f"""
                Analyze the collected blog data quality and completeness:
                
                Total blogs collected: {len(collected_data_for_all_blogs)}
                Sample data: {collected_data_for_all_blogs[0] if collected_data_for_all_blogs else {}}
                
                Evaluate:
                1. Data completeness (% of fields filled)
                2. Data accuracy assessment 
                3. Blog diversity and quality
                4. Areas for improvement
                5. Overall collection success rate
                
                Return JSON with analysis results.
                """
                
                analysis_messages = [{"role": "user", "content": final_analysis_prompt}]
                final_analysis = self.llm_handler.chat_with_ollama_for_tools(analysis_messages, [])
                analysis_result = final_analysis.get("content", "{}")
                
                try:
                    analysis_data = json.loads(analysis_result)
                    computed_quality_score = analysis_data.get("overall_success_rate", quality_score)
                except json.JSONDecodeError:
                    computed_quality_score = quality_score
                    analysis_data = {"raw_analysis": analysis_result}
            else:
                computed_quality_score = 0
                analysis_data = {"message": "ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

            return json.dumps({
                "status": "success",
                "final_blog_count": len(collected_data_for_all_blogs),
                "all_done_by_llm": all_done,
                "quality_score": computed_quality_score,
                "quality_analysis": analysis_data,
                "recommendations": recommendations,
                "message": f"ëª¨ë“  ë¸”ë¡œê·¸ ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. í’ˆì§ˆ ì ìˆ˜: {computed_quality_score}/10" if collected_data_for_all_blogs else "ìˆ˜ì§‘ëœ ë¸”ë¡œê·¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            })

        else:
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬ ìš”ì²­: {tool_name}")
            return json.dumps({
                "status": "error",
                "message": f"ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬ '{tool_name}' ì…ë‹ˆë‹¤."
            })

    async def run_agent_for_keywords(self, initial_keywords: list):
        self._update_status("ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸ ì‹œì‘...")
        final_structured_blog_data = []  # ìµœì¢… ìˆ˜ì§‘ ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

        # ê°œì„ ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (get_improved_system_prompt ì§ì ‘ ì‚¬ìš©)
        system_prompt = get_improved_system_prompt(settings.DATA_FIELDS_TO_EXTRACT)

        messages_history = [{"role": "system", "content": system_prompt}]

        user_query = f"ë‹¤ìŒ í‚¤ì›Œë“œì— ëŒ€í•œ ë¸”ë¡œê·¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”: {', '.join(initial_keywords)}. ê° ë¸”ë¡œê·¸ì—ì„œ {', '.join(settings.DATA_FIELDS_TO_EXTRACT)} ì •ë³´ë¥¼ ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤."
        messages_history.append({"role": "user", "content": user_query})

        max_turns = settings.AGENT_MAX_TURNS

        try:
            # ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì‹œë„
            try:
                await self.browser_controller._ensure_browser()
                self._update_status("ğŸŒ ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì™„ë£Œ.")
            except RuntimeError as e_browser:  # Playwright ë“œë¼ì´ë²„ ë¯¸ì„¤ì¹˜ ë“± Runtime ì—ëŸ¬
                self._update_status(f"âš ï¸ ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e_browser)}")
                self._update_status("Playwright ë¸Œë¼ìš°ì € ë“œë¼ì´ë²„ ìë™ ì„¤ì¹˜ë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
                try:
                    process = await asyncio.create_subprocess_exec(
                        "python", "-m", "playwright", "install", "--with-deps",  # '--with-deps'ë¡œ í•„ìš”í•œ ëª¨ë“  ë¸Œë¼ìš°ì € ì„¤ì¹˜
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE
                    )
                    stdout, stderr = await process.communicate()
                    if process.returncode == 0:
                        self._update_status("Playwright ë¸Œë¼ìš°ì € ì„¤ì¹˜ ì„±ê³µ. ë‹¤ì‹œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
                        await self.browser_controller._ensure_browser()
                        self._update_status("ğŸŒ ë¸Œë¼ìš°ì € ì¬ì´ˆê¸°í™” ì™„ë£Œ.")
                    else:
                        error_message = stderr.decode(errors='ignore') if stderr else "ì•Œ ìˆ˜ ì—†ëŠ” ì„¤ì¹˜ ì˜¤ë¥˜"
                        self._update_status(f"âš ï¸ Playwright ë¸Œë¼ìš°ì € ìë™ ì„¤ì¹˜ ì‹¤íŒ¨: {error_message}")
                        raise RuntimeError(f"Playwright ë¸Œë¼ìš°ì € ì„¤ì¹˜ ì‹¤íŒ¨ í›„ ì´ˆê¸°í™” ë¶ˆê°€: {error_message}")
                except Exception as e_install:
                    self._update_status(f"âš ï¸ Playwright ë¸Œë¼ìš°ì € ì„¤ì¹˜ í”„ë¡œì„¸ìŠ¤ ì¤‘ ì˜¤ë¥˜: {str(e_install)}")
                    raise RuntimeError(f"Playwright í™˜ê²½ ì„¤ì • ì‹¤íŒ¨: {str(e_install)}")

            for turn_count in range(max_turns):
                self._update_status(f"ì—ì´ì „íŠ¸ ì‘ì—… {turn_count + 1}/{max_turns}ë²ˆì§¸ í„´ ì§„í–‰ ì¤‘...")

                if final_structured_blog_data:
                    self._update_status(f"í˜„ì¬ê¹Œì§€ {len(final_structured_blog_data)}ê°œì˜ ë¸”ë¡œê·¸ ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    self._update_status(f"ì•„ì§ ìˆ˜ì§‘ëœ ë¸”ë¡œê·¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ì§‘ ì‹œë„ ì¤‘...")

                assistant_response_message = self.llm_handler.chat_with_ollama_for_tools(
                    messages_history,
                    TOOLS_SPEC
                )
                messages_history.append(assistant_response_message)

                if assistant_response_message.get("content"):
                    content = assistant_response_message['content']
                    self._update_status(f"ğŸ¤– LLM ì‘ë‹µ: {content[:200]}...")  # ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ í‘œì‹œ

                tool_calls = assistant_response_message.get("tool_calls")

                # LLMì´ tool_calls ëŒ€ì‹  contentì— JSON í˜•íƒœë¡œ ë„êµ¬ í˜¸ì¶œì„ ë°˜í™˜í•˜ëŠ” ê²½ìš° ì²˜ë¦¬
                if not tool_calls and assistant_response_message.get("content"):
                    content = assistant_response_message['content']
                    logger.info(f"tool_callsê°€ ì—†ìŠµë‹ˆë‹¤. contentì—ì„œ JSON í˜•íƒœ ë„êµ¬ í˜¸ì¶œ ê²€ìƒ‰ ì¤‘...")
                    logger.debug(f"LLM content (ì „ì²´): {content}")

                    # ê°œì„ ëœ JSON íŒŒì‹± ë¡œì§ ì‚¬ìš©
                    def robust_json_parse(json_string):
                        # 1ë‹¨ê³„: í‘œì¤€ JSON íŒŒì‹±
                        try:
                            return json.loads(json_string)
                        except json.JSONDecodeError:
                            pass
                        # 2ë‹¨ê³„: single quotesë¥¼ double quotesë¡œ ë³€í™˜
                        try:
                            json_compatible = json_string.replace("'", '"')
                            return json.loads(json_compatible)
                        except json.JSONDecodeError:
                            pass
                        # 3ë‹¨ê³„: ast.literal_eval ì‚¬ìš©
                        try:
                            import ast
                            return ast.literal_eval(json_string)
                        except (ValueError, SyntaxError):
                            pass
                        # 4ë‹¨ê³„: ì •ê·œì‹ìœ¼ë¡œ JSON ì¶”ì¶œ í›„ ì¬ì‹œë„
                        json_pattern = r'(\{[\s\S]*\})'
                        json_match = re.search(json_pattern, json_string)
                        if json_match:
                            json_str_cleaned = json_match.group(1).replace("'", '"')
                            try:
                                return json.loads(json_str_cleaned)
                            except json.JSONDecodeError:
                                pass
                        return None

                    parsed_tool_call_from_content = None
                    content_cleaned_for_json = content.strip()

                    # 1. ë§ˆí¬ë‹¤ìš´ JSON ë¸”ë¡ ì‹œë„
                    markdown_match = re.search(r'```json\s*(\{[\s\S]*?\})\s*```', content_cleaned_for_json, re.DOTALL)
                    if markdown_match:
                        json_str_from_content = markdown_match.group(1)
                        logger.info(f"ë§ˆí¬ë‹¤ìš´ JSON ë¸”ë¡ì—ì„œ ë‚´ìš© ì¶”ì¶œ: {json_str_from_content}")
                        parsed_tool_call_from_content = robust_json_parse(json_str_from_content)
                        if parsed_tool_call_from_content:
                            logger.info(f"ë§ˆí¬ë‹¤ìš´ JSON ë¸”ë¡ íŒŒì‹± ì„±ê³µ: {parsed_tool_call_from_content}")
                        else:
                            logger.error(f"ë§ˆí¬ë‹¤ìš´ JSON ë¸”ë¡ íŒŒì‹± ì‹¤íŒ¨")

                    # 2. ë§ˆí¬ë‹¤ìš´ì´ ì—†ê±°ë‚˜ ì‹¤íŒ¨ ì‹œ, ì „ì²´ contentê°€ JSONì¸ì§€ ì‹œë„
                    if not parsed_tool_call_from_content and \
                            content_cleaned_for_json.startswith('{'):
                        logger.info(f"ì „ì²´ contentê°€ JSON í˜•íƒœì¼ ê°€ëŠ¥ì„±. íŒŒì‹± ì‹œë„...")
                        parsed_tool_call_from_content = robust_json_parse(content_cleaned_for_json)
                        if parsed_tool_call_from_content:
                            logger.info(f"ì „ì²´ content JSON íŒŒì‹± ì„±ê³µ: {type(parsed_tool_call_from_content)}")
                        else:
                            logger.error(f"ì „ì²´ content JSON íŒŒì‹± ì‹¤íŒ¨")

                    # 3. JSONì´ ë„ˆë¬´ ê¸¸ì–´ì„œ ì˜ë¦° ê²½ìš° ì •ê·œì‹ìœ¼ë¡œ extract_blog_fields_from_text íŠ¹ë³„ ì²˜ë¦¬
                    if not parsed_tool_call_from_content and 'extract_blog_fields_from_text' in content:
                        logger.info(f"extract_blog_fields_from_text ë„êµ¬ ê°ì§€. íŠ¹ë³„ íŒŒì‹± ì‹œë„...")
                        # text_content ì¶”ì¶œ (í°ë”°ì˜´í‘œ ì•ˆì˜ ë‚´ìš©)
                        text_match = re.search(r'"text_content"\s*:\s*"([^"]*(?:\\.[^"]*)*)', content)
                        # original_url ì¶”ì¶œ - ë” í¬ê´„ì ì¸ íŒ¨í„´ìœ¼ë¡œ ì‹œë„
                        url_match = re.search(r'"(?:original_)?url"\s*:\s*"([^"]+)"', content)
                        
                        # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì—ì„œ ìµœê·¼ ì›¹í˜ì´ì§€ URL ì°¾ê¸°
                        recent_url = None
                        for msg in reversed(messages_history):
                            if (msg.get("role") == "tool" and 
                                msg.get("name") == "get_webpage_content_and_interact"):
                                try:
                                    tool_result = json.loads(msg.get("content", "{}"))
                                    if tool_result.get("status") == "success":
                                        recent_url = tool_result.get("url") or tool_result.get("final_url")
                                        break
                                except json.JSONDecodeError:
                                    continue
                        
                        if text_match:
                            text_content = text_match.group(1)
                            # ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì ì²˜ë¦¬
                            text_content = text_content.replace('\\n', '\n').replace('\\"', '"')
                            # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì˜ë¼ë‚´ê¸°
                            if len(text_content) > 2000:
                                text_content = text_content[:2000] + "...[TRUNCATED]"
                            
                            # URL ìš°ì„ ìˆœìœ„: JSONì—ì„œ ì¶”ì¶œ > ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ > ê¸°ë³¸ê°’
                            if url_match:
                                original_url = url_match.group(1)
                            elif recent_url:
                                original_url = recent_url
                                logger.info(f"ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì—ì„œ URL ë³µêµ¬: {original_url}")
                            else:
                                original_url = "unknown_url"
                            
                            parsed_tool_call_from_content = {
                                'name': 'extract_blog_fields_from_text',
                                'parameters': {
                                    'text_content': text_content,
                                    'original_url': original_url
                                }
                            }
                            logger.info(f"extract_blog_fields_from_text íŠ¹ë³„ íŒŒì‹± ì„±ê³µ, URL: {original_url}")
                        else:
                            logger.warning(f"extract_blog_fields_from_text ë„êµ¬ëŠ” ê°ì§€ë˜ì—ˆìœ¼ë‚˜ text_content ì¶”ì¶œ ì‹¤íŒ¨")

                    if parsed_tool_call_from_content and \
                            'name' in parsed_tool_call_from_content and \
                            'parameters' in parsed_tool_call_from_content:
                        tool_name_from_content = parsed_tool_call_from_content['name']
                        tool_args_from_content = parsed_tool_call_from_content['parameters']

                        is_valid_tool = any(
                            tool_spec['function']['name'] == tool_name_from_content for tool_spec in TOOLS_SPEC)
                        if is_valid_tool:
                            fake_tool_call = {
                                "id": f"call_from_content_{abs(hash(json.dumps(tool_args_from_content)))}",
                                "type": "function",
                                "function": {
                                    "name": tool_name_from_content,
                                    "arguments": json.dumps(tool_args_from_content)
                                }
                            }
                            tool_calls = [fake_tool_call]  # ìƒì„±ëœ ê°€ìƒ tool_callë¡œ ëŒ€ì²´
                            logger.info(f"âœ… LLM contentì—ì„œ ë„êµ¬ í˜¸ì¶œ ê°ì§€ ë° ë³€í™˜ ì„±ê³µ: {tool_name_from_content}")
                            self._update_status(f"ğŸ”§ LLM contentì—ì„œ ë„êµ¬ í˜¸ì¶œ ê°ì§€: {tool_name_from_content}")
                        else:
                            logger.warning(f"LLM contentì—ì„œ ê°ì§€ëœ ë„êµ¬ '{tool_name_from_content}'ê°€ TOOLS_SPECì— ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    elif parsed_tool_call_from_content:
                        logger.warning(
                            f"content JSON íŒŒì‹±ì€ ì„±ê³µí–ˆìœ¼ë‚˜, 'name' ë˜ëŠ” 'parameters' í•„ë“œê°€ ëˆ„ë½: {parsed_tool_call_from_content}")

                if not tool_calls:  # ìœ„ ë¡œì§ìœ¼ë¡œë„ tool_callsë¥¼ ë§Œë“¤ì§€ ëª»í–ˆë‹¤ë©´ ì¢…ë£Œ
                    self._update_status("LLMì´ ë” ì´ìƒ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê±°ë‚˜ ì‘ì—…ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
                    break  # ë‹¤ìŒ í„´ìœ¼ë¡œ ë„˜ì–´ê°€ì§€ ì•Šê³  ë£¨í”„ ì¢…ë£Œ

                for tool_call in tool_calls:
                    tool_id = tool_call["id"]
                    tool_function = tool_call["function"]
                    tool_name = tool_function["name"]

                    try:
                        # tool_function["arguments"]ëŠ” LLMì—ì„œ ì˜¨ ê²ƒì´ë¯€ë¡œ ë¬¸ìì—´ë¡œ ê°€ì •
                        if not isinstance(tool_function["arguments"], str):
                            logger.error(f"ë„êµ¬ '{tool_name}'ì˜ ì¸ìê°€ ë¬¸ìì—´ì´ ì•„ë‹™ë‹ˆë‹¤: {type(tool_function['arguments'])}")
                            # ë°©ì–´ì ìœ¼ë¡œ ë¬¸ìì—´ë¡œ ë³€í™˜ ì‹œë„ (LLMì´ ê°ì²´ë¥¼ ì§ì ‘ ì¤„ ê²½ìš° ëŒ€ë¹„)
                            tool_args_str = json.dumps(tool_function["arguments"])
                        else:
                            tool_args_str = tool_function["arguments"]
                        tool_args = json.loads(tool_args_str)

                    except json.JSONDecodeError:
                        logger.error(f"ë„êµ¬ '{tool_name}' ì¸ì JSON ë””ì½”ë”© ì‹¤íŒ¨: {tool_function['arguments']}")
                        tool_result_content = f"ì˜¤ë¥˜: ë„êµ¬ '{tool_name}'ì˜ ì¸ì íŒŒì‹± ì‹¤íŒ¨."
                        messages_history.append({"role": "tool", "tool_call_id": tool_id, "name": tool_name,
                                                 "content": tool_result_content})
                        continue  # ë‹¤ìŒ tool_callë¡œ ë„˜ì–´ê°

                    # ì‹¤ì œ ë„êµ¬ ì‹¤í–‰
                    tool_result = await self._execute_tool_call(tool_name, tool_args, final_structured_blog_data, messages_history)

                    messages_history.append({
                        "role": "tool",
                        "tool_call_id": tool_id,
                        "name": tool_name,
                        "content": tool_result  # _execute_tool_callì€ JSON ë¬¸ìì—´ì„ ë°˜í™˜
                    })
                    self._update_status(f"ğŸ› ï¸ ë„êµ¬ '{tool_name}' ì‹¤í–‰ ê²°ê³¼ ìˆ˜ì‹ .")

                    try:
                        tool_result_obj = json.loads(tool_result)  # tool_resultëŠ” JSON ë¬¸ìì—´
                    except (json.JSONDecodeError, TypeError):
                        logger.warning(f"ë„êµ¬ '{tool_name}' ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ì‚¬ìš©.")
                        tool_result_obj = {"status": "unknown", "message": tool_result}

                    # finalize_blog_data_collection ë„êµ¬ í˜¸ì¶œ ì‹œ ë°ì´í„° ì €ì¥ ë° ì¢…ë£Œ ì²˜ë¦¬
                    if tool_name == "finalize_blog_data_collection" and \
                            tool_result_obj.get("status") == "success":
                        self._update_status("LLMì´ ëª¨ë“  ì‘ì—… ì™„ë£Œë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

                        if not final_structured_blog_data and tool_result_obj.get("final_blog_count", 0) > 0:
                            logger.warning("LLMì€ ë°ì´í„°ê°€ ìˆë‹¤ê³  ë³´ê³ í–ˆìœ¼ë‚˜, ë‚´ë¶€ ë¦¬ìŠ¤íŠ¸ëŠ” ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì´ì „ ë‹¨ê³„ í™•ì¸ í•„ìš”.")

                        # final_structured_blog_dataì— ì‹¤ì œë¡œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸ í›„ ì €ì¥
                        if final_structured_blog_data:
                            self._update_status(f"âœ… ì´ {len(final_structured_blog_data)}ê°œì˜ ë¸”ë¡œê·¸ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.")
                            output_filepath = self.data_writer.save_data(
                                final_structured_blog_data,
                                "agent_blog_data"  # íŒŒì¼ëª… ì ‘ë‘ì‚¬
                            )
                            self._update_status(f"âœ… ìµœì¢… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_filepath}")
                            await self.browser_controller._maybe_close_browser(force_close=True)
                            return output_filepath  # ì„±ê³µì ìœ¼ë¡œ íŒŒì¼ ì €ì¥ í›„ ì¢…ë£Œ
                        else:
                            self._update_status("âš ï¸ ìˆ˜ì§‘ëœ ë¸”ë¡œê·¸ ë°ì´í„°ê°€ ì—†ì–´ íŒŒì¼ì„ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                            await self.browser_controller._maybe_close_browser(force_close=True)
                            return None  # ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìœ¼ë¯€ë¡œ None ë°˜í™˜

            # ìµœëŒ€ í„´ ë„ë‹¬ ì‹œ
            self._update_status(f"ìµœëŒ€ ì‘ì—… í„´({max_turns})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. í˜„ì¬ê¹Œì§€ì˜ ì •ë³´ë¡œ ë§ˆë¬´ë¦¬í•©ë‹ˆë‹¤.")
            # ë°ì´í„° ë³µêµ¬ ë¡œì§ì€ final_structured_blog_dataê°€ ë¹„ì–´ìˆì„ ë•Œë§Œ ì‘ë™í•˜ë„ë¡ finally ë¸”ë¡ ì´í›„ë¡œ ì´ë™

        except RuntimeError as e_browser_runtime:  # ë¸Œë¼ìš°ì € ì´ˆê¸°í™”/ì„¤ì¹˜ ê´€ë ¨ ì—ëŸ¬
            logger.error(f"ë¸Œë¼ìš°ì € ê´€ë ¨ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e_browser_runtime}", exc_info=True)
            self._update_status(
                f"âŒ ë¸Œë¼ìš°ì € ì˜¤ë¥˜: {e_browser_runtime}. Playwrightê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš” (ì˜ˆ: python -m playwright install).")
            return None  # ì¹˜ëª…ì  ì˜¤ë¥˜ë¡œ ê°„ì£¼í•˜ê³  None ë°˜í™˜
        except Exception as e:
            logger.error(f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            self._update_status(f"âŒ ì—ì´ì „íŠ¸ ì˜¤ë¥˜: {e}")
            self._update_status(f"ì˜¤ë¥˜ ìƒì„¸ ì •ë³´ (ë””ë²„ê¹…ìš©): {traceback.format_exc()[:1000]}")  # ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ ìë¦„
            # ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ finally ë¸”ë¡ì€ ì‹¤í–‰ë¨

        finally:
            # ëª¨ë“  ì‘ì—…(ì„±ê³µ, ì˜ˆì™¸, ìµœëŒ€ í„´ ë„ë‹¬) í›„ ë¸Œë¼ìš°ì € í™•ì‹¤íˆ ë‹«ê¸°
            await self.browser_controller._maybe_close_browser(force_close=True)
            self._update_status("ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ.")

        # ë£¨í”„ ì •ìƒ ì¢…ë£Œ(break) ë˜ëŠ” ìµœëŒ€ í„´ ë„ë‹¬ ì‹œ, ë˜ëŠ” ì˜ˆì™¸ ë°œìƒ í›„ finallyë¥¼ ê±°ì³ ì´ ë¶€ë¶„ ì‹¤í–‰
        # ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°, ë¶€ë¶„ ì €ì¥ ì‹œë„
        if final_structured_blog_data:
            output_filepath = self.data_writer.save_data(final_structured_blog_data, "agent_blog_data_partial")
            self._update_status(f" ë¶€ë¶„ ë°ì´í„° ì €ì¥ (íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ): {output_filepath}")
            return output_filepath
        else:  # ë°ì´í„°ê°€ ì „í˜€ ì—†ëŠ” ê²½ìš° (finalizeê°€ í˜¸ì¶œë˜ì§€ ì•Šì•˜ê±°ë‚˜, í˜¸ì¶œë˜ì—ˆì–´ë„ ë°ì´í„°ê°€ ì—†ì—ˆê±°ë‚˜, ì¤‘ê°„ì— ì˜¤ë¥˜)
            # ì¶”ê°€ëœ ë¶€ë¶„: ë°ì´í„°ê°€ ë¹„ì—ˆì§€ë§Œ LLMì´ ì´ì „ì— URLì„ ì°¾ì•˜ëŠ”ì§€ í™•ì¸
            if not final_structured_blog_data:  # ë‹¤ì‹œ í•œë²ˆ í™•ì¸ (ìœ„ì—ì„œ ì €ì¥í–ˆì„ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ)
                self._update_status("ìµœì¢… ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì—ì„œ URL ê²€ìƒ‰ ì‹œë„...")
                urls_found_in_history = set()
                for msg in messages_history:
                    if msg.get("role") == "tool" and msg.get("name") == "search_web_for_blogs":
                        try:
                            search_tool_result_content = msg.get("content", "{}")
                            search_tool_result = json.loads(search_tool_result_content)
                            if search_tool_result.get("status") == "success" and search_tool_result.get("found_urls"):
                                urls_found_in_history.update(search_tool_result.get("found_urls", []))
                        except (json.JSONDecodeError, TypeError):
                            logger.warning(f"ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì˜ search_web_for_blogs ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {msg.get('content')}")

                if urls_found_in_history:
                    self._update_status(
                        f"{len(urls_found_in_history)}ê°œì˜ URLì´ ê²€ìƒ‰ë˜ì—ˆìœ¼ë‚˜ ì™„ì „í•œ ë°ì´í„° ì¶”ì¶œì€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. URLë§Œì´ë¼ë„ ì €ì¥í•©ë‹ˆë‹¤.")
                    for url_item in urls_found_in_history:
                        final_structured_blog_data.append({
                            "blog_name": "ì¶”ì¶œ ì‹¤íŒ¨ - URLë§Œ í™•ë³´",
                            "url": url_item,
                            "blog_id": "extraction-failed-url-only",
                            "post_title": "N/A",
                            "recent_post_date": "N/A",
                            "total_posts": "N/A"
                        })
                    if final_structured_blog_data:  # URL ì •ë³´ë¼ë„ ìˆë‹¤ë©´ ì €ì¥
                        output_filepath = self.data_writer.save_data(final_structured_blog_data,
                                                                     "agent_blog_data_urls_only")
                        self._update_status(f" URL ì •ë³´ë§Œ ì €ì¥ ì™„ë£Œ: {output_filepath}")
                        return output_filepath

        return None  # ëª¨ë“  ê²½ìš°ì— í•´ë‹¹í•˜ì§€ ì•Šìœ¼ë©´ None ë°˜í™˜